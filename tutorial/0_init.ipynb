{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import json \n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from src.agents import instanciate_agent\n",
    "from src.environment import Environment\n",
    "from src.get_data import load_data\n",
    "from src.run import Run\n",
    "from src.utilities import create_directory_tree, instanciate_scaler, prepare_initial_portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    'initial_cash': None,\n",
    "    'initial_portfolio': '../portfolios_and_tickers/initial_portfolio.json',\n",
    "    'assets_to_trade': '../portfolios_and_tickers/tickers_S&P500_dummy.txt',\n",
    "    'buy_rule': 'most_first',\n",
    "    'buy_cost': 0.001,\n",
    "    'sell_cost': 0.001,\n",
    "    'bank_rate': 0.5,\n",
    "    'initial_date': '2014-12-31',\n",
    "    'final_date': '2020-12-30',\n",
    "    'limit_n_stocks': 20,\n",
    "    'agent_type': 'distributional',\n",
    "    'sac_temperature': 2.0,\n",
    "    'gamma': 0.99,\n",
    "    'lr_Q': 0.0003,\n",
    "    'lr_pi': 0.0003,\n",
    "    'lr_alpha': 0.0003,\n",
    "    'tau': 0.005,\n",
    "    'batch_size': 32,\n",
    "    'memory_size': 100,\n",
    "    'grad_clip': 1.0,\n",
    "    'delay': 1,\n",
    "    'layer_size': 256,\n",
    "    'n_episodes': 2,\n",
    "    'mode': 'train',\n",
    "    'experimental': False,\n",
    "    'checkpoint_directory': None,\n",
    "    'plot': True,\n",
    "    'seed': 42,\n",
    "    'gpu_devices': [0, 1, 2, 3],\n",
    "    'use_corr_matrix': True,\n",
    "    'use_corr_eigenvalues': False,\n",
    "    'window': 20,\n",
    "    'number_of_eigenvalues': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "\n",
    "# Convert the dictionary to Namespace object\n",
    "args_namespace = Namespace(**args_dict)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES environment variable to an empty string\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA (GPU) is available.\")\n",
    "else:\n",
    "    print(\"CUDA (GPU) is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>>>> Reading the data <<<<<\n",
      ">>>>> Reading the provided initial portfolio <<<<<\n",
      ">>>>> Appending the correlation matrix <<<<<\n",
      ">>>>> Running <<<<<\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m main\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_namespace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ssd2tb/work/1_backend/2_quant_trading/Portfolio-Optimization-and-Goal-Based-Investment-with-Reinforcement-Learning/tutorial/../src/main.py:103\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# running the training or testing, saving plots\u001b[39;00m\n\u001b[1;32m    102\u001b[0m initial_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 103\u001b[0m \u001b[43mrun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mplot:\n\u001b[1;32m    105\u001b[0m     run\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mgenerate_plots()\n",
      "File \u001b[0;32m/mnt/ssd2tb/work/1_backend/2_quant_trading/Portfolio-Optimization-and-Goal-Based-Investment-with-Reinforcement-Learning/tutorial/../src/run.py:82\u001b[0m, in \u001b[0;36mRun.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>> Running <<<<<\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_episodes):\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_one_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39msave_logs()\n",
      "File \u001b[0;32m/mnt/ssd2tb/work/1_backend/2_quant_trading/Portfolio-Optimization-and-Goal-Based-Investment-with-Reinforcement-Learning/tutorial/../src/run.py:103\u001b[0m, in \u001b[0;36mRun._run_one_episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m     portfolio_content_history \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnumber_of_shares]\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m--> 103\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     observation_, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    105\u001b[0m     observation_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mtransform([observation_])[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/mnt/ssd2tb/work/1_backend/2_quant_trading/Portfolio-Optimization-and-Goal-Based-Investment-with-Reinforcement-Learning/tutorial/../src/agents.py:168\u001b[0m, in \u001b[0;36mAgent.choose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Choose an action to take given an observation of the state of the environment.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    action (np.array) taken in the input state \u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([observation])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 168\u001b[0m actions, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreparameterize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/mnt/ssd2tb/work/1_backend/2_quant_trading/Portfolio-Optimization-and-Goal-Based-Investment-with-Reinforcement-Learning/tutorial/../src/networks.py:197\u001b[0m, in \u001b[0;36mActor.sample\u001b[0;34m(self, state, reparameterize)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    184\u001b[0m            state: np\u001b[38;5;241m.\u001b[39mndarray, \n\u001b[1;32m    185\u001b[0m            reparameterize: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    186\u001b[0m            ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mtensor]:\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sample from the Normal distribution, output of feedforward method, to give an action\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        action sampled from Normal disribution, as well as log probability of the given action\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m     mu, sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m     normal \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mNormal(mu, sigma)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reparameterize:\n",
      "File \u001b[0;32m/mnt/ssd2tb/work/1_backend/2_quant_trading/Portfolio-Optimization-and-Goal-Based-Investment-with-Reinforcement-Learning/tutorial/../src/networks.py:170\u001b[0m, in \u001b[0;36mActor.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    159\u001b[0m             state: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m    160\u001b[0m             ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[torch\u001b[38;5;241m.\u001b[39mtensor]:\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement the feedforward of the net.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m        expectation and standard deviation of a Normal distribution\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mgelu(x)\n\u001b[1;32m    172\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/finlab/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/finlab/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "from src.main import main\n",
    "\n",
    "main(args_namespace)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
